"""
ECGä»¿çœŸæ•°æ®éªŒè¯å·¥å…· V25
ç”¨é€”ï¼šæ£€æŸ¥ç”Ÿæˆçš„ä»¿çœŸæ•°æ®è´¨é‡ï¼Œç¡®ä¿æ ‡ç­¾ä¸å›¾åƒå¯¹é½
"""

import numpy as np
import pandas as pd
import cv2
import os
import json
import matplotlib.pyplot as plt
from pathlib import Path
import random

# ============================
# 1. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
# ============================
def check_data_integrity(output_dir):
    """æ£€æŸ¥æ‰€æœ‰æ ·æœ¬æ˜¯å¦å®Œæ•´"""
    print("=" * 70)
    print("æ•°æ®å®Œæ•´æ€§æ£€æŸ¥")
    print("=" * 70)
    
    all_samples = [d for d in os.listdir(output_dir) 
                   if os.path.isdir(os.path.join(output_dir, d)) and d.startswith('0')]
    
    print(f"æ‰¾åˆ° {len(all_samples)} ä¸ªæ ·æœ¬ç›®å½•\n")
    
    incomplete_samples = []
    required_files = ['_dirty.png', '_label_grid.png', '_label_wave.png',
                     '_label_other.png', '_label_baseline.png', '_metadata.json']
    
    for sample_id in all_samples:
        sample_dir = os.path.join(output_dir, sample_id)
        missing_files = []
        
        for suffix in required_files:
            filepath = os.path.join(sample_dir, f"{sample_id}{suffix}")
            if not os.path.exists(filepath):
                missing_files.append(suffix)
        
        if missing_files:
            incomplete_samples.append((sample_id, missing_files))
    
    if incomplete_samples:
        print(f"âš ï¸  å‘ç° {len(incomplete_samples)} ä¸ªä¸å®Œæ•´çš„æ ·æœ¬:")
        for sample_id, missing in incomplete_samples[:10]:
            print(f"  {sample_id}: ç¼ºå¤± {missing}")
        if len(incomplete_samples) > 10:
            print(f"  ... è¿˜æœ‰ {len(incomplete_samples) - 10} ä¸ª")
    else:
        print("âœ… æ‰€æœ‰æ ·æœ¬æ–‡ä»¶å®Œæ•´")
    
    return len(incomplete_samples) == 0

# ============================
# 2. æ ‡ç­¾å¯¹é½æ£€æŸ¥
# ============================
def check_label_alignment(output_dir, num_samples=10):
    """æ£€æŸ¥æ ‡ç­¾ä¸å›¾åƒæ˜¯å¦å¯¹é½"""
    print("\n" + "=" * 70)
    print("æ ‡ç­¾å¯¹é½æ£€æŸ¥")
    print("=" * 70)
    
    all_samples = [d for d in os.listdir(output_dir)
                   if os.path.isdir(os.path.join(output_dir, d)) and d.startswith('0')]
    
    if len(all_samples) == 0:
        print("âš ï¸  æœªæ‰¾åˆ°æ ·æœ¬")
        return False
    
    # éšæœºé€‰æ‹©æ ·æœ¬
    selected = random.sample(all_samples, min(num_samples, len(all_samples)))
    
    alignment_issues = []
    
    for sample_id in selected:
        sample_dir = os.path.join(output_dir, sample_id)
        
        dirty_img = cv2.imread(os.path.join(sample_dir, f"{sample_id}_dirty.png"))
        wave_mask = cv2.imread(os.path.join(sample_dir, f"{sample_id}_label_wave.png"), 0)
        
        if dirty_img is None or wave_mask is None:
            continue
        
        # æ£€æŸ¥å°ºå¯¸æ˜¯å¦åŒ¹é…
        if dirty_img.shape[:2] != wave_mask.shape[:2]:
            alignment_issues.append((sample_id, "å°ºå¯¸ä¸åŒ¹é…"))
            continue
        
        # æ£€æŸ¥æ³¢å½¢æ©ç æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
        wave_pixels = np.sum(wave_mask > 0)
        total_pixels = wave_mask.shape[0] * wave_mask.shape[1]
        wave_ratio = wave_pixels / total_pixels
        
        if wave_ratio < 0.005 or wave_ratio > 0.3:
            alignment_issues.append((sample_id, f"æ³¢å½¢å æ¯”å¼‚å¸¸: {wave_ratio:.3f}"))
    
    if alignment_issues:
        print(f"âš ï¸  å‘ç° {len(alignment_issues)} ä¸ªå¯¹é½é—®é¢˜:")
        for sample_id, issue in alignment_issues:
            print(f"  {sample_id}: {issue}")
    else:
        print(f"âœ… æ£€æŸ¥çš„ {len(selected)} ä¸ªæ ·æœ¬æ ‡ç­¾å¯¹é½æ­£å¸¸")
    
    return len(alignment_issues) == 0

# ============================
# 3. ç‰©ç†å‚æ•°éªŒè¯
# ============================
def validate_physical_params(output_dir, num_samples=50):
    """éªŒè¯ç‰©ç†å‚æ•°çš„åˆ†å¸ƒ"""
    print("\n" + "=" * 70)
    print("ç‰©ç†å‚æ•°åˆ†å¸ƒéªŒè¯")
    print("=" * 70)
    
    all_samples = [d for d in os.listdir(output_dir)
                   if os.path.isdir(os.path.join(output_dir, d)) and d.startswith('0')]
    
    selected = random.sample(all_samples, min(num_samples, len(all_samples)))
    
    paper_speeds = []
    gains = []
    px_per_mms = []
    
    for sample_id in selected:
        metadata_path = os.path.join(output_dir, sample_id, f"{sample_id}_metadata.json")
        if not os.path.exists(metadata_path):
            continue
        
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        params = metadata['physical_params']
        paper_speeds.append(params['paper_speed_mm_s'])
        gains.append(params['gain_mm_mv'])
        px_per_mms.append(params['px_per_mm'])
    
    print(f"æ£€æŸ¥äº† {len(paper_speeds)} ä¸ªæ ·æœ¬çš„ç‰©ç†å‚æ•°\n")
    
    # çº¸é€Ÿåˆ†å¸ƒ
    print("çº¸é€Ÿåˆ†å¸ƒ (mm/s):")
    paper_speed_counts = {}
    for speed in paper_speeds:
        paper_speed_counts[speed] = paper_speed_counts.get(speed, 0) + 1
    for speed, count in sorted(paper_speed_counts.items()):
        print(f"  {speed:5.1f} mm/s: {count:3d} ({count/len(paper_speeds)*100:5.1f}%)")
    
    # å¢ç›Šåˆ†å¸ƒ
    print("\nå¢ç›Šåˆ†å¸ƒ (mm/mV):")
    gain_counts = {}
    for gain in gains:
        gain_counts[gain] = gain_counts.get(gain, 0) + 1
    for gain, count in sorted(gain_counts.items()):
        print(f"  {gain:5.1f} mm/mV: {count:3d} ({count/len(gains)*100:5.1f}%)")
    
    # åˆ†è¾¨ç‡ç»Ÿè®¡
    print(f"\nåˆ†è¾¨ç‡ (px/mm):")
    print(f"  æœ€å°: {min(px_per_mms):.2f}")
    print(f"  æœ€å¤§: {max(px_per_mms):.2f}")
    print(f"  å¹³å‡: {np.mean(px_per_mms):.2f}")
    
    # éªŒè¯æ˜¯å¦ç¬¦åˆé¢„æœŸ
    valid = True
    if set(paper_speeds) != {25.0, 50.0}:
        print("âš ï¸  çº¸é€Ÿä¸åœ¨é¢„æœŸèŒƒå›´ [25.0, 50.0]")
        valid = False
    if set(gains) != {5.0, 10.0, 20.0}:
        print("âš ï¸  å¢ç›Šä¸åœ¨é¢„æœŸèŒƒå›´ [5.0, 10.0, 20.0]")
        valid = False
    if min(px_per_mms) < 18.0 or max(px_per_mms) > 22.0:
        print("âš ï¸  åˆ†è¾¨ç‡ä¸åœ¨é¢„æœŸèŒƒå›´ [18.0, 22.0]")
        valid = False
    
    if valid:
        print("\nâœ… ç‰©ç†å‚æ•°åˆ†å¸ƒæ­£å¸¸")
    
    return valid

# ============================
# 4. å¸ƒå±€åˆ†å¸ƒæ£€æŸ¥
# ============================
def check_layout_distribution(output_dir):
    """æ£€æŸ¥å¸ƒå±€ç±»å‹çš„åˆ†å¸ƒ"""
    print("\n" + "=" * 70)
    print("å¸ƒå±€åˆ†å¸ƒæ£€æŸ¥")
    print("=" * 70)
    
    all_samples = [d for d in os.listdir(output_dir)
                   if os.path.isdir(os.path.join(output_dir, d)) and d.startswith('0')]
    
    layout_counts = {}
    degradation_counts = {}
    
    for sample_id in all_samples:
        metadata_path = os.path.join(output_dir, sample_id, f"{sample_id}_metadata.json")
        if not os.path.exists(metadata_path):
            continue
        
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        layout = metadata['layout_type']
        degradation = metadata['degradation_type']
        
        layout_counts[layout] = layout_counts.get(layout, 0) + 1
        degradation_counts[degradation] = degradation_counts.get(degradation, 0) + 1
    
    total = len(all_samples)
    
    print(f"æ£€æŸ¥äº† {total} ä¸ªæ ·æœ¬\n")
    
    print("å¸ƒå±€åˆ†å¸ƒ:")
    for layout, count in sorted(layout_counts.items()):
        print(f"  {layout:15s}: {count:5d} ({count/total*100:5.1f}%)")
    
    print("\né€€åŒ–åˆ†å¸ƒ:")
    for degradation, count in sorted(degradation_counts.items()):
        print(f"  {degradation:15s}: {count:5d} ({count/total*100:5.1f}%)")
    
    return True

# ============================
# 5. å¯è§†åŒ–æ£€æŸ¥
# ============================
def visualize_samples(output_dir, num_samples=4, save_path=None):
    """å¯è§†åŒ–æ ·æœ¬å’Œæ ‡ç­¾"""
    print("\n" + "=" * 70)
    print("ç”Ÿæˆå¯è§†åŒ–")
    print("=" * 70)
    
    all_samples = [d for d in os.listdir(output_dir)
                   if os.path.isdir(os.path.join(output_dir, d)) and d.startswith('0')]
    
    if len(all_samples) == 0:
        print("âš ï¸  æœªæ‰¾åˆ°æ ·æœ¬")
        return
    
    selected = random.sample(all_samples, min(num_samples, len(all_samples)))
    
    fig, axes = plt.subplots(num_samples, 5, figsize=(20, 4*num_samples))
    if num_samples == 1:
        axes = axes.reshape(1, -1)
    
    for idx, sample_id in enumerate(selected):
        sample_dir = os.path.join(output_dir, sample_id)
        
        # è¯»å–å›¾åƒ
        dirty_img = cv2.imread(os.path.join(sample_dir, f"{sample_id}_dirty.png"))
        grid_img = cv2.imread(os.path.join(sample_dir, f"{sample_id}_label_grid.png"))
        wave_mask = cv2.imread(os.path.join(sample_dir, f"{sample_id}_label_wave.png"), 0)
        other_mask = cv2.imread(os.path.join(sample_dir, f"{sample_id}_label_other.png"), 0)
        baseline_mask = cv2.imread(os.path.join(sample_dir, f"{sample_id}_label_baseline.png"), 0)
        
        # è¯»å–å…ƒæ•°æ®
        metadata_path = os.path.join(sample_dir, f"{sample_id}_metadata.json")
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        # è½¬æ¢é¢œè‰²
        dirty_img = cv2.cvtColor(dirty_img, cv2.COLOR_BGR2RGB)
        grid_img = cv2.cvtColor(grid_img, cv2.COLOR_BGR2RGB)
        
        # æ˜¾ç¤º
        axes[idx, 0].imshow(dirty_img)
        axes[idx, 0].set_title(f"Dirty Image\n{metadata['layout_type']}\n{metadata['degradation_type']}")
        axes[idx, 0].axis('off')
        
        axes[idx, 1].imshow(grid_img)
        axes[idx, 1].set_title("Grid Label")
        axes[idx, 1].axis('off')
        
        axes[idx, 2].imshow(wave_mask, cmap='hot')
        axes[idx, 2].set_title("Wave Mask")
        axes[idx, 2].axis('off')
        
        axes[idx, 3].imshow(other_mask, cmap='hot')
        axes[idx, 3].set_title("Other Mask")
        axes[idx, 3].axis('off')
        
        axes[idx, 4].imshow(baseline_mask, cmap='hot')
        axes[idx, 4].set_title("Baseline Mask")
        axes[idx, 4].axis('off')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        print(f"âœ… å¯è§†åŒ–å·²ä¿å­˜è‡³: {save_path}")
    else:
        plt.show()

# ============================
# 6. å¯¼è”æ—¶åºéªŒè¯
# ============================
def validate_lead_timing(output_dir, original_csv_dir, num_samples=10):
    """éªŒè¯å¯¼è”æ—¶åºæ˜¯å¦æ­£ç¡®ï¼ˆLead II=10s, å…¶ä»–=2.5sï¼‰"""
    print("\n" + "=" * 70)
    print("å¯¼è”æ—¶åºéªŒè¯")
    print("=" * 70)
    
    all_samples = [d for d in os.listdir(output_dir)
                   if os.path.isdir(os.path.join(output_dir, d)) and d.startswith('0')]
    
    selected = random.sample(all_samples, min(num_samples, len(all_samples)))
    
    timing_issues = []
    
    for sample_id in selected:
        # è§£æ ecg_id
        parts = sample_id.split('_')
        ecg_id = parts[0]
        
        # è¯»å–åŸå§‹CSV
        csv_path = os.path.join(original_csv_dir, ecg_id, f"{ecg_id}.csv")
        if not os.path.exists(csv_path):
            continue
        
        df = pd.read_csv(csv_path)
        fs = len(df) / 10.0  # é‡‡æ ·ç‡
        
        # è¯»å–å…ƒæ•°æ®
        metadata_path = os.path.join(output_dir, sample_id, f"{sample_id}_metadata.json")
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        # æ£€æŸ¥é…ç½®
        expected_lead_II = metadata['physical_params']['lead_II_duration_s']
        expected_others = metadata['physical_params']['other_leads_duration_s']
        
        if expected_lead_II != 10.0:
            timing_issues.append((sample_id, f"Lead II æ—¶é•¿é”™è¯¯: {expected_lead_II}"))
        if expected_others != 2.5:
            timing_issues.append((sample_id, f"å…¶ä»–å¯¼è”æ—¶é•¿é”™è¯¯: {expected_others}"))
    
    if timing_issues:
        print(f"âš ï¸  å‘ç° {len(timing_issues)} ä¸ªæ—¶åºé—®é¢˜:")
        for sample_id, issue in timing_issues:
            print(f"  {sample_id}: {issue}")
    else:
        print(f"âœ… æ£€æŸ¥çš„ {len(selected)} ä¸ªæ ·æœ¬æ—¶åºé…ç½®æ­£ç¡®")
    
    return len(timing_issues) == 0

# ============================
# 7. ä¸»éªŒè¯æµç¨‹
# ============================
def run_full_validation(output_dir, original_csv_dir=None, save_viz=True):
    """è¿è¡Œå®Œæ•´éªŒè¯æµç¨‹"""
    print("\n")
    print("â•”" + "â•" * 68 + "â•—")
    print("â•‘" + " " * 20 + "ECG ä»¿çœŸæ•°æ®éªŒè¯æŠ¥å‘Š" + " " * 28 + "â•‘")
    print("â•š" + "â•" * 68 + "â•")
    
    results = {}
    
    # 1. æ•°æ®å®Œæ•´æ€§
    results['integrity'] = check_data_integrity(output_dir)
    
    # 2. æ ‡ç­¾å¯¹é½
    results['alignment'] = check_label_alignment(output_dir, num_samples=20)
    
    # 3. ç‰©ç†å‚æ•°
    results['physics'] = validate_physical_params(output_dir, num_samples=100)
    
    # 4. å¸ƒå±€åˆ†å¸ƒ
    results['layout'] = check_layout_distribution(output_dir)
    
    # 5. æ—¶åºéªŒè¯ï¼ˆå¦‚æœæä¾›äº†åŸå§‹CSVç›®å½•ï¼‰
    if original_csv_dir:
        results['timing'] = validate_lead_timing(output_dir, original_csv_dir, num_samples=20)
    
    # 6. å¯è§†åŒ–
    if save_viz:
        viz_path = os.path.join(output_dir, "validation_visualization.png")
        visualize_samples(output_dir, num_samples=4, save_path=viz_path)
    
    # æ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 70)
    print("éªŒè¯æ±‡æ€»")
    print("=" * 70)
    
    all_passed = all(results.values())
    
    for test_name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"{test_name.upper():20s}: {status}")
    
    print("\n" + "=" * 70)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰éªŒè¯é€šè¿‡ï¼æ•°æ®è´¨é‡è‰¯å¥½")
    else:
        print("âš ï¸  éƒ¨åˆ†éªŒè¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¸Šè¿°é—®é¢˜")
    print("=" * 70)
    
    return all_passed

# ============================
# ä½¿ç”¨ç¤ºä¾‹
# ============================
if __name__ == "__main__":
    # é…ç½®è·¯å¾„
    OUTPUT_DIR = "/Volumes/movie/work/physionet-ecg-image-digitization-simulations-V25"
    ORIGINAL_CSV_DIR = "/Volumes/movie/work/physionet-ecg-image-digitization/train"
    
    # è¿è¡ŒéªŒè¯
    run_full_validation(
        output_dir=OUTPUT_DIR,
        original_csv_dir=ORIGINAL_CSV_DIR,
        save_viz=True
    )