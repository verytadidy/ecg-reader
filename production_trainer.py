#!/usr/bin/env python3
"""
ç”Ÿäº§å°±ç»ªçš„ECGé‡å»ºæ¨¡å‹è®­ç»ƒè„šæœ¬

ç‰¹ç‚¹:
1. âœ… æ”¯æŒå¤šé‡‡æ ·ç‡ï¼ˆé‡é‡‡æ ·åˆ°500Hzï¼‰
2. âœ… å®Œå–„çš„æ—¥å¿—å’Œæ£€æŸ¥ç‚¹ç®¡ç†
3. âœ… è‡ªåŠ¨è®¾å¤‡æ£€æµ‹ï¼ˆCUDA/MPS/CPUï¼‰
4. âœ… TensorBoardå¯è§†åŒ–
5. âœ… æ—©åœå’Œå­¦ä¹ ç‡è°ƒåº¦
6. âœ… æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¯é€‰ï¼‰
"""

import os
import sys
import argparse
import json
from pathlib import Path
from datetime import datetime

import torch
import torch.nn as nn
from torch.utils.tensorboard import SummaryWriter
from tqdm import tqdm
import numpy as np

# å¯¼å…¥æ¨¡å‹å’Œæ•°æ®é›†
from ecg_model import ECGReconstructionModel
from production_dataset import create_dataloaders


class ProductionTrainer:
    """ç”Ÿäº§çº§è®­ç»ƒå™¨"""
    def __init__(self, args):
        self.args = args
        self.device = self._setup_device()
        self.output_dir = self._setup_output_dir()
        
        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        print("\n" + "="*70)
        print("åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨...")
        print("="*70)
        self.train_loader, self.val_loader = create_dataloaders(
            sim_root=args.sim_root,
            csv_root=args.csv_root,
            batch_size=args.batch_size,
            num_workers=args.num_workers,
            train_split=args.train_split,
            target_fs=args.target_fs,
            max_samples=args.max_samples if args.debug else None,
            cache_in_memory=args.cache
        )
        
        # åˆ›å»ºæ¨¡å‹
        print("="*70)
        print("åˆå§‹åŒ–æ¨¡å‹...")
        print("="*70)
        
        # ğŸ”¥ Mac M2 MPSå…¼å®¹ï¼šç¦ç”¨STNé¿å…grid_sampleré—®é¢˜
        enable_stn = not (self.device.type == 'mps')
        if not enable_stn:
            print("âš ï¸  æ£€æµ‹åˆ°MPSè®¾å¤‡ï¼Œç¦ç”¨STNï¼ˆgrid_samplerä¸æ”¯æŒï¼‰")
        
        self.model = ECGReconstructionModel(
            num_leads=12,
            signal_length=args.target_fs * 10,
            pretrained=args.pretrained,
            enable_stn=enable_stn  # MPSä¸Šç¦ç”¨STN
        ).to(self.device)
        
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        print(f"æ¨¡å‹å‚æ•°é‡: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print()
        
        # æŸå¤±å‡½æ•°
        self.criterion = self._create_loss_function()
        
        # ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=args.lr,
            weight_decay=args.weight_decay,
            betas=(0.9, 0.999)
        )
        
        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
            self.optimizer,
            mode='min',
            factor=0.5,
            patience=5,
            verbose=True,
            min_lr=1e-6
        )
        
        # TensorBoard
        self.writer = SummaryWriter(log_dir=self.output_dir / 'tensorboard')
        
        # è®­ç»ƒçŠ¶æ€
        self.start_epoch = 0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        
        # æ¢å¤è®­ç»ƒ
        if args.resume:
            self._load_checkpoint(args.resume)
        
        # ä¿å­˜é…ç½®
        self._save_config()
    
    def _setup_device(self):
        """è®¾ç½®è®­ç»ƒè®¾å¤‡"""
        if self.args.force_cpu:
            device = torch.device('cpu')
            device_name = 'CPU'
        elif torch.cuda.is_available():
            device = torch.device('cuda')
            device_name = f'CUDA ({torch.cuda.get_device_name(0)})'
        elif torch.backends.mps.is_available():
            device = torch.device('mps')
            device_name = 'MPS (Apple Silicon)'
        else:
            device = torch.device('cpu')
            device_name = 'CPU'
        
        print("\n" + "="*70)
        print(f"è®­ç»ƒè®¾å¤‡: {device_name}")
        print("="*70)
        
        return device
    
    def _setup_output_dir(self):
        """è®¾ç½®è¾“å‡ºç›®å½•"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(self.args.output_dir) / f"run_{timestamp}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        (output_dir / 'checkpoints').mkdir(exist_ok=True)
        (output_dir / 'logs').mkdir(exist_ok=True)
        
        print(f"è¾“å‡ºç›®å½•: {output_dir}")
        return output_dir
    
    def _create_loss_function(self):
        """åˆ›å»ºæŸå¤±å‡½æ•°"""
        from ecg_trainer import MultiTaskLoss
        
        return MultiTaskLoss(loss_weights={
            'seg': self.args.loss_weight_seg,
            'grid': self.args.loss_weight_grid,
            'baseline': self.args.loss_weight_baseline,
            'theta': self.args.loss_weight_theta,
            'signal': self.args.loss_weight_signal
        }).to(self.device)
    
    def _save_config(self):
        """ä¿å­˜è®­ç»ƒé…ç½®"""
        config = {
            'model': {
                'num_leads': 12,
                'signal_length': self.args.target_fs * 10,
                'pretrained': self.args.pretrained
            },
            'data': {
                'sim_root': self.args.sim_root,
                'csv_root': self.args.csv_root,
                'target_fs': self.args.target_fs,
                'target_size': [512, 672],
                'train_split': self.args.train_split
            },
            'training': {
                'batch_size': self.args.batch_size,
                'epochs': self.args.epochs,
                'lr': self.args.lr,
                'weight_decay': self.args.weight_decay,
                'early_stopping_patience': self.args.patience
            },
            'loss_weights': {
                'seg': self.args.loss_weight_seg,
                'grid': self.args.loss_weight_grid,
                'baseline': self.args.loss_weight_baseline,
                'theta': self.args.loss_weight_theta,
                'signal': self.args.loss_weight_signal
            }
        }
        
        with open(self.output_dir / 'config.json', 'w') as f:
            json.dump(config, f, indent=2)
    
    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        
        total_losses = {
            'total': 0, 'seg': 0, 'grid': 0,
            'baseline': 0, 'theta': 0, 'signal': 0
        }
        num_batches = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {epoch}/{self.args.epochs}")
        
        for batch_idx, batch in enumerate(pbar):
            try:
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                images = batch['image'].to(self.device, non_blocking=True)
                wave_seg = batch['wave_seg'].to(self.device, non_blocking=True)
                grid_mask = batch['grid_mask'].to(self.device, non_blocking=True)
                baseline_heatmaps = batch['baseline_heatmaps'].to(self.device, non_blocking=True)
                theta_gt = batch['theta_gt'].to(self.device, non_blocking=True)
                gt_signal = batch['gt_signal'].to(self.device, non_blocking=True)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(images)
                
                # è®¡ç®—æŸå¤±
                targets = {
                    'wave_seg': wave_seg,
                    'grid_mask': grid_mask,
                    'baseline_heatmaps': baseline_heatmaps,
                    'theta_gt': theta_gt,
                    'gt_signal': gt_signal
                }
                losses = self.criterion(outputs, targets)
                
                # ğŸ”¥ æ£€æŸ¥NaN
                if torch.isnan(losses['total']) or torch.isinf(losses['total']):
                    print(f"\nâš ï¸  æ£€æµ‹åˆ°NaN/InfæŸå¤±ï¼Œè·³è¿‡æ­¤batch")
                    print(f"  Losses: {[(k, v.item()) for k, v in losses.items()]}")
                    # æ£€æŸ¥å“ªä¸ªè¾“å‡ºæœ‰é—®é¢˜
                    for k, v in outputs.items():
                        if isinstance(v, torch.Tensor):
                            if torch.isnan(v).any():
                                print(f"  âœ— {k} åŒ…å«NaN")
                            elif torch.isinf(v).any():
                                print(f"  âœ— {k} åŒ…å«Inf")
                    continue
                
                # åå‘ä¼ æ’­
                self.optimizer.zero_grad()
                losses['total'].backward()
                
                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                
                self.optimizer.step()
                
                # ç´¯ç§¯æŸå¤±
                for key in total_losses.keys():
                    total_losses[key] += losses[key].item()
                num_batches += 1
                
                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'loss': f"{losses['total'].item():.4f}",
                    'sig': f"{losses['signal'].item():.4f}",
                    'lr': f"{self.optimizer.param_groups[0]['lr']:.2e}"
                })
                
            except Exception as e:
                print(f"\nâš ï¸  Batch {batch_idx} å‡ºé”™: {e}")
                if self.args.debug:
                    raise e
                continue
        
        # è®¡ç®—å¹³å‡æŸå¤±
        avg_losses = {k: v / num_batches for k, v in total_losses.items()}
        
        # è®°å½•åˆ°TensorBoard
        for key, value in avg_losses.items():
            self.writer.add_scalar(f'Train/{key}_loss', value, epoch)
        self.writer.add_scalar('Train/lr', self.optimizer.param_groups[0]['lr'], epoch)
        
        return avg_losses
    
    @torch.no_grad()
    def validate(self, epoch):
        """éªŒè¯"""
        self.model.eval()
        
        total_losses = {
            'total': 0, 'seg': 0, 'grid': 0,
            'baseline': 0, 'theta': 0, 'signal': 0
        }
        num_batches = 0
        
        pbar = tqdm(self.val_loader, desc="Validating")
        
        for batch in pbar:
            try:
                images = batch['image'].to(self.device, non_blocking=True)
                wave_seg = batch['wave_seg'].to(self.device, non_blocking=True)
                grid_mask = batch['grid_mask'].to(self.device, non_blocking=True)
                baseline_heatmaps = batch['baseline_heatmaps'].to(self.device, non_blocking=True)
                theta_gt = batch['theta_gt'].to(self.device, non_blocking=True)
                gt_signal = batch['gt_signal'].to(self.device, non_blocking=True)
                
                outputs = self.model(images)
                
                targets = {
                    'wave_seg': wave_seg,
                    'grid_mask': grid_mask,
                    'baseline_heatmaps': baseline_heatmaps,
                    'theta_gt': theta_gt,
                    'gt_signal': gt_signal
                }
                losses = self.criterion(outputs, targets)
                
                for key in total_losses.keys():
                    total_losses[key] += losses[key].item()
                num_batches += 1
                
            except Exception as e:
                print(f"\nâš ï¸  éªŒè¯å‡ºé”™: {e}")
                if self.args.debug:
                    raise e
                continue
        
        avg_losses = {k: v / num_batches for k, v in total_losses.items()}
        
        # è®°å½•åˆ°TensorBoard
        for key, value in avg_losses.items():
            self.writer.add_scalar(f'Val/{key}_loss', value, epoch)
        
        return avg_losses
    
    def save_checkpoint(self, epoch, val_loss, is_best=False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'val_loss': val_loss,
            'best_val_loss': self.best_val_loss,
            'config': self.args.__dict__
        }
        
        # ä¿å­˜æœ€æ–°
        torch.save(checkpoint, self.output_dir / 'checkpoints' / 'last.pth')
        
        # ä¿å­˜æœ€ä½³
        if is_best:
            torch.save(checkpoint, self.output_dir / 'checkpoints' / 'best.pth')
            print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹ (val_loss: {val_loss:.4f})")
        
        # å®šæœŸä¿å­˜
        if epoch % self.args.save_freq == 0:
            torch.save(checkpoint, self.output_dir / 'checkpoints' / f'epoch_{epoch}.pth')
    
    def _load_checkpoint(self, checkpoint_path):
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        self.start_epoch = checkpoint['epoch'] + 1
        self.best_val_loss = checkpoint['best_val_loss']
        
        print(f"âœ“ ä» {checkpoint_path} æ¢å¤è®­ç»ƒ")
        print(f"  Epoch: {checkpoint['epoch']}")
        print(f"  Best Val Loss: {checkpoint['best_val_loss']:.4f}")
    
    def train(self):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print("\n" + "="*70)
        print("å¼€å§‹è®­ç»ƒ")
        print("="*70)
        
        for epoch in range(self.start_epoch, self.args.epochs):
            # è®­ç»ƒ
            train_losses = self.train_epoch(epoch)
            
            # éªŒè¯
            val_losses = self.validate(epoch)
            
            # å­¦ä¹ ç‡è°ƒæ•´
            self.scheduler.step(val_losses['total'])
            
            # æ‰“å°ç»Ÿè®¡
            print(f"\nEpoch {epoch} Summary:")
            print(f"  Train Loss: {train_losses['total']:.4f} (signal: {train_losses['signal']:.4f})")
            print(f"  Val Loss:   {val_losses['total']:.4f} (signal: {val_losses['signal']:.4f})")
            print(f"  Best Val:   {self.best_val_loss:.4f}")
            print(f"  LR:         {self.optimizer.param_groups[0]['lr']:.2e}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = val_losses['total'] < self.best_val_loss
            if is_best:
                self.best_val_loss = val_losses['total']
                self.patience_counter = 0
            else:
                self.patience_counter += 1
            
            self.save_checkpoint(epoch, val_losses['total'], is_best)
            
            # æ—©åœ
            if self.patience_counter >= self.args.patience:
                print(f"\næ—©åœè§¦å‘ï¼{self.args.patience} epochsæ— æ”¹å–„")
                break
        
        # è®­ç»ƒç»“æŸ
        self.writer.close()
        print("\n" + "="*70)
        print("è®­ç»ƒå®Œæˆï¼")
        print(f"æœ€ä½³éªŒè¯æŸå¤±: {self.best_val_loss:.4f}")
        print(f"è¾“å‡ºç›®å½•: {self.output_dir}")
        print("="*70)


def main():
    parser = argparse.ArgumentParser(description='ECGé‡å»ºæ¨¡å‹è®­ç»ƒï¼ˆç”Ÿäº§ç‰ˆï¼‰')
    
    # æ•°æ®å‚æ•°
    parser.add_argument('--sim_root', type=str, required=True, help='ä»¿çœŸæ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--csv_root', type=str, required=True, help='åŸå§‹CSVæ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--target_fs', type=int, default=500, help='ç›®æ ‡é‡‡æ ·ç‡ï¼ˆé‡é‡‡æ ·ï¼‰')
    parser.add_argument('--train_split', type=float, default=0.9, help='è®­ç»ƒé›†æ¯”ä¾‹')
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=100, help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch_size', type=int, default=4, help='æ‰¹å¤§å°')
    parser.add_argument('--lr', type=float, default=1e-4, help='å­¦ä¹ ç‡')
    parser.add_argument('--weight_decay', type=float, default=1e-4, help='æƒé‡è¡°å‡')
    parser.add_argument('--num_workers', type=int, default=4, help='æ•°æ®åŠ è½½workeræ•°')
    
    # æŸå¤±æƒé‡
    parser.add_argument('--loss_weight_seg', type=float, default=1.0)
    parser.add_argument('--loss_weight_grid', type=float, default=0.5)
    parser.add_argument('--loss_weight_baseline', type=float, default=0.8)
    parser.add_argument('--loss_weight_theta', type=float, default=0.3)
    parser.add_argument('--loss_weight_signal', type=float, default=2.0)
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--pretrained', action='store_true', help='ä½¿ç”¨é¢„è®­ç»ƒæƒé‡')
    
    # å…¶ä»–
    parser.add_argument('--output_dir', type=str, default='./experiments', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--resume', type=str, default=None, help='æ¢å¤è®­ç»ƒæ£€æŸ¥ç‚¹')
    parser.add_argument('--save_freq', type=int, default=10, help='ä¿å­˜æ£€æŸ¥ç‚¹é¢‘ç‡')
    parser.add_argument('--patience', type=int, default=15, help='æ—©åœpatience')
    parser.add_argument('--force_cpu', action='store_true', help='å¼ºåˆ¶CPUè®­ç»ƒ')
    parser.add_argument('--cache', action='store_true', help='ç¼“å­˜æ•°æ®åˆ°å†…å­˜')
    parser.add_argument('--debug', action='store_true', help='è°ƒè¯•æ¨¡å¼')
    parser.add_argument('--max_samples', type=int, default=None, help='æœ€å¤§æ ·æœ¬æ•°ï¼ˆè°ƒè¯•ç”¨ï¼‰')
    
    args = parser.parse_args()
    
    # è°ƒè¯•æ¨¡å¼é…ç½®
    if args.debug:
        args.epochs = 3
        args.max_samples = 100
        args.save_freq = 1
        print("\nâš ï¸  è°ƒè¯•æ¨¡å¼å·²å¯ç”¨")
    
    # åˆ›å»ºè®­ç»ƒå™¨å¹¶å¼€å§‹è®­ç»ƒ
    trainer = ProductionTrainer(args)
    
    try:
        trainer.train()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        print(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜è‡³: {trainer.output_dir / 'checkpoints'}")
    except Exception as e:
        print(f"\n\nâŒ è®­ç»ƒå‡ºé”™: {e}")
        if args.debug:
            raise e


if __name__ == '__main__':
    # ä½¿ç”¨ç¤ºä¾‹:
    #
    # å¿«é€Ÿæµ‹è¯•ï¼ˆMac M2ï¼‰:
    #   python production_trainer.py \
    #       --sim_root /path/to/simulations \
    #       --csv_root /path/to/train \
    #       --batch_size 2 \
    #       --num_workers 0 \
    #       --debug
    #
    # å®Œæ•´è®­ç»ƒï¼ˆGPUï¼‰:
    #   python production_trainer.py \
    #       --sim_root /path/to/simulations \
    #       --csv_root /path/to/train \
    #       --batch_size 16 \
    #       --num_workers 8 \
    #       --pretrained \
    #       --epochs 100
    #
    # æ¢å¤è®­ç»ƒ:
    #   python production_trainer.py \
    #       --sim_root /path/to/simulations \
    #       --csv_root /path/to/train \
    #       --resume ./experiments/run_xxx/checkpoints/best.pth
    
    main()