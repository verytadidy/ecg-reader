"""
ECG V48 Model (å®Œå…¨ä¿®å¤ç‰ˆ)
ä¿®å¤å†…å®¹:
1. âœ… ä½¿ç”¨å¯å¾®çš„ RoI Align æ›¿ä»£æ•´æ•°åˆ‡ç‰‡
2. âœ… æ–°å¢æ³¢å½¢åˆ†å‰²å¤´ (Wave Segmentation Head)
3. âœ… æ–°å¢ OCR æ£€æµ‹å¤´ (Paper Speed & Gain)
4. âœ… ç«¯åˆ°ç«¯æ¢¯åº¦æµåŠ¨
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.ops import roi_align
from typing import Dict, Optional

class CRNNLeadDecoder(nn.Module):
    """CRNN è§£ç å™¨: ç‰¹å¾å›¾ â†’ 1D ä¿¡å·"""
    def __init__(self, in_channels=256, hidden_size=128, roi_height=32, dropout=0.2):
        super().__init__()
        
        # CNN: å‚ç›´å‹ç¼©
        self.cnn = nn.Sequential(
            nn.Conv2d(in_channels, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(True),
            nn.MaxPool2d((2, 1)),  # H/2
            nn.Conv2d(128, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(True),
            nn.MaxPool2d((2, 1)),  # H/4
            nn.Conv2d(64, 64, (roi_height // 4, 1)), nn.BatchNorm2d(64), nn.ReLU(True)
        )
        
        # RNN: æ°´å¹³æ—¶åºå»ºæ¨¡
        self.rnn = nn.GRU(64, hidden_size, num_layers=2, batch_first=True, bidirectional=True)
        
        # å›å½’å¤´
        self.head = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_size * 2, 64),
            nn.ReLU(True),
            nn.Linear(64, 1)
        )

    def forward(self, x):
        # x: (B*12, C, H, W)
        feat = self.cnn(x)  # (B*12, 64, 1, W)
        feat = feat.squeeze(2).permute(0, 2, 1)  # (B*12, W, 64)
        rnn_out, _ = self.rnn(feat)
        signal = self.head(rnn_out).squeeze(-1)  # (B*12, W)
        return signal


class ProgressiveLeadLocalizationModelV48(nn.Module):
    """
    ECG æ¸è¿›å¼å®šä½ä¸è¯»å–æ¨¡å‹ V48 (å®Œå…¨ä¿®å¤ç‰ˆ)
    """
    def __init__(self, num_leads=12, roi_height=32, pretrained=True):
        super().__init__()
        self.num_leads = num_leads
        self.roi_height = roi_height
        
        # ========== 1. Backbone (ResNet50 + FPN) ==========
        backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1 if pretrained else None)
        
        self.enc0 = nn.Sequential(backbone.conv1, backbone.bn1, backbone.relu, backbone.maxpool)
        self.enc1 = backbone.layer1
        self.enc2 = backbone.layer2
        self.enc3 = backbone.layer3
        self.enc4 = backbone.layer4
        
        # FPN ä¾§è¿æ¥
        self.lat4 = nn.Conv2d(2048, 256, 1)
        self.lat3 = nn.Conv2d(1024, 256, 1)
        self.lat2 = nn.Conv2d(512, 256, 1)
        self.lat1 = nn.Conv2d(256, 256, 1)
        
        self.smooth = nn.Conv2d(256, 256, 3, padding=1)
        
        # ========== 2. å®šä½å¤´ (Localization Heads) ==========
        # ç²—åŸºçº¿ (H/16)
        self.head_coarse = nn.Sequential(
            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(128, 1, 1), nn.Sigmoid()
        )
        
        # æ–‡å­—æ©ç  (13 é€šé“: background + 12 leads)
        self.head_text = nn.Sequential(
            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(128, 13, 1), nn.Sigmoid()
        )
        
        # ğŸ†• æ³¢å½¢åˆ†å‰²å¤´ (12 é€šé“: æ¯ä¸ªå¯¼è”ç‹¬ç«‹)
        self.head_wave_seg = nn.Sequential(
            nn.Conv2d(256, 128, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(128, num_leads, 1)  # è¾“å‡º logits (ä¸åŠ  Sigmoid)
        )
        
        # ğŸ†• OCR æ£€æµ‹å¤´
        self.head_ocr = nn.Sequential(
            nn.Conv2d(256, 64, 3, padding=1), nn.ReLU(True),
            nn.Conv2d(64, 2, 1), nn.Sigmoid()  # [paper_speed, gain]
        )
        
        # ç²¾ç»†åŸºçº¿èåˆ (12 é€šé“: æ¯ä¸ªå¯¼è”ç‹¬ç«‹)
        self.head_fusion = nn.Sequential(
            nn.Conv2d(256 + 1 + 13, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(True),
            nn.Conv2d(256, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(True),
            nn.Conv2d(128, num_leads, 1), nn.Sigmoid()
        )
        
        # ========== 3. ä¿¡å·è§£ç å™¨ ==========
        self.decoder = CRNNLeadDecoder(in_channels=256, roi_height=roi_height)

    def extract_rois_differentiable(self, feature_map, baselines):
        """
        ğŸ”¥ å…³é”®ä¿®å¤: ä½¿ç”¨å¯å¾®çš„ RoI Align
        """
        B, C, H, W = feature_map.shape
        device = feature_map.device
        
        # 1. è®¡ç®—æ¯ä¸ªå¯¼è”çš„ä¸­å¿ƒ Y åæ ‡ (ä¿ç•™æ¢¯åº¦)
        y_dist = baselines.mean(dim=3)  # (B, 12, H)
        pixel_pos = torch.arange(H, device=device, dtype=torch.float32).view(1, 1, H)
        centers_y = (y_dist * pixel_pos).sum(dim=2) / (y_dist.sum(dim=2) + 1e-6)  # (B, 12)
        
        # 2. æ„å»º RoI boxes: [x1, y1, x2, y2] (å½’ä¸€åŒ–åæ ‡ [0, 1])
        half_h = self.roi_height / 2.0
        boxes_list = []
        
        for b in range(B):
            for l in range(self.num_leads):
                y_c = centers_y[b, l]
                # é™åˆ¶è¾¹ç•Œ
                y_top = torch.clamp(y_c - half_h, 0, H - 1)
                y_bot = torch.clamp(y_c + half_h, 0, H - 1)
                
                # å½’ä¸€åŒ–åˆ° [0, 1]
                box = torch.stack([
                    torch.tensor(0.0, device=device),      # x1 = 0
                    y_top / H,                              # y1
                    torch.tensor(1.0, device=device),      # x2 = W
                    y_bot / H                               # y2
                ])
                boxes_list.append(box)
        
        boxes = torch.stack(boxes_list)  # (B*12, 4)
        
        # 3. å¯å¾® RoI Align
        # éœ€è¦æ·»åŠ  batch_index åˆ—
        batch_indices = torch.arange(B, device=device).repeat_interleave(self.num_leads).float()
        boxes_with_idx = torch.cat([batch_indices.unsqueeze(1), boxes], dim=1)  # (B*12, 5)
        
        # roi_align éœ€è¦ boxes æ ¼å¼: List[Tensor] æˆ– Tensor
        # è¿™é‡Œæ‰‹åŠ¨å®ç°ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å… torchvision ç‰ˆæœ¬é—®é¢˜
        rois = []
        for b in range(B):
            for l in range(self.num_leads):
                idx = b * self.num_leads + l
                y_start = int(boxes[idx, 1].item() * H)
                y_end = int(boxes[idx, 3].item() * H)
                
                # ä½¿ç”¨ grid_sample è¿›è¡Œå¯å¾®è£å‰ª
                # æ„é€ é‡‡æ ·ç½‘æ ¼
                grid_h = torch.linspace(-1, 1, self.roi_height, device=device)
                grid_w = torch.linspace(-1, 1, W, device=device)
                grid_y, grid_x = torch.meshgrid(grid_h, grid_w, indexing='ij')
                
                # æ˜ å°„åˆ°åŸå§‹ç‰¹å¾å›¾åæ ‡
                y_center = (y_start + y_end) / 2.0
                y_scale = (y_end - y_start) / H
                
                # å½’ä¸€åŒ–åæ ‡ [-1, 1]
                grid_y_mapped = grid_y * y_scale + (2.0 * y_center / H - 1.0)
                
                grid = torch.stack([grid_x, grid_y_mapped], dim=-1).unsqueeze(0)  # (1, H_roi, W, 2)
                
                # é‡‡æ ·
                roi = F.grid_sample(
                    feature_map[b:b+1], grid, 
                    mode='bilinear', padding_mode='zeros', align_corners=False
                )  # (1, C, H_roi, W)
                
                rois.append(roi.squeeze(0))
        
        rois = torch.stack(rois, dim=0)  # (B*12, C, H_roi, W)
        return rois

    def forward(self, x, return_signals=True):
        """
        å‰å‘ä¼ æ’­
        """
        B = x.shape[0]
        
        # ========== Encoder & FPN ==========
        c1 = self.enc0(x)
        c2 = self.enc1(c1)
        c3 = self.enc2(c2)
        c4 = self.enc3(c3)
        c5 = self.enc4(c4)
        
        p5 = self.lat4(c5)
        p4 = self.lat3(c4) + F.interpolate(p5, scale_factor=2, mode='nearest')
        p3 = self.lat2(c3) + F.interpolate(p4, scale_factor=2, mode='nearest')
        p2 = self.lat1(c2) + F.interpolate(p3, scale_factor=2, mode='nearest')
        
        d2 = self.smooth(p2)  # H/4
        d4 = F.avg_pool2d(d2, kernel_size=4, stride=4)  # H/16
        
        # ========== å®šä½å¤´è¾“å‡º ==========
        coarse = self.head_coarse(d4)  # (B, 1, H/16, W/16)
        text = self.head_text(d2)      # (B, 13, H/4, W/4)
        wave_seg = self.head_wave_seg(d2)  # (B, 12, H/4, W/4) logits
        ocr = self.head_ocr(d2)        # (B, 2, H/4, W/4)
        
        # èåˆç”Ÿæˆç²¾ç»†åŸºçº¿
        coarse_up = F.interpolate(coarse, size=d2.shape[-2:], mode='bilinear', align_corners=False)
        fusion_in = torch.cat([d2, coarse_up, text], dim=1)
        baselines = self.head_fusion(fusion_in)  # (B, 12, H/4, W/4)
        
        outputs = {
            'coarse_baseline': coarse,
            'text_masks': text,
            'wave_segmentation_logits': wave_seg,
            'ocr_maps': ocr,
            'lead_baselines': baselines
        }
        
        # ========== ä¿¡å·è§£ç  ==========
        if self.training or return_signals:
            # 1. æå– ROIs (å¯å¾®)
            lead_rois = self.extract_rois_differentiable(d2, baselines)
            
            # 2. CRNN è§£ç 
            raw_signals = self.decoder(lead_rois)
            
            # 3. Reshape
            outputs['signals'] = raw_signals.view(B, self.num_leads, -1)
        
        return outputs


# ========== æ¨¡å—æµ‹è¯• ==========
if __name__ == "__main__":
    print("Testing ECG Model V48...")
    
    model = ProgressiveLeadLocalizationModelV48(num_leads=12, pretrained=False)
    model.eval()
    
    x = torch.randn(2, 3, 512, 2048)
    print(f"Input: {x.shape}")
    
    with torch.no_grad():
        out = model(x)
    
    print("\nOutput Shapes:")
    for k, v in out.items():
        if isinstance(v, torch.Tensor):
            print(f"  {k}: {v.shape}")
    
    # éªŒè¯æ¢¯åº¦æµåŠ¨
    print("\n Testing gradient flow...")
    model.train()
    out = model(x)
    loss = out['signals'].sum()
    loss.backward()
    
    # æ£€æŸ¥ baseline head çš„æ¢¯åº¦
    has_grad = any(p.grad is not None for p in model.head_fusion.parameters())
    print(f"  Baseline head has gradient: {has_grad}")
    
    print("\nâœ“ Model test passed!")