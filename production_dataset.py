"""
ç”Ÿäº§å°±ç»ªçš„ECGæ•°æ®é›†åŠ è½½å™¨

ç‰¹ç‚¹:
1. âœ… æ”¯æŒå¤šé‡‡æ ·ç‡ï¼ˆé‡é‡‡æ ·åˆ°500Hzç»Ÿä¸€å¤„ç†ï¼‰
2. âœ… ç›´æ¥ä»ä»¿çœŸå™¨è¾“å‡ºåŠ è½½ï¼ˆæ— éœ€é¢„å¤„ç†ï¼‰
3. âœ… é«˜æ•ˆç¼“å­˜æœºåˆ¶ï¼ˆå¯é€‰ï¼‰
4. âœ… å®Œå–„çš„é”™è¯¯å¤„ç†
5. âœ… æ•°æ®éªŒè¯
"""

import os
import json
import numpy as np
import cv2
from pathlib import Path
import torch
from torch.utils.data import Dataset
import torch.nn.functional as F
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pandas as pd
from typing import Optional, Tuple, Dict
import warnings


class ECGProductionDataset(Dataset):
    """
    ç”Ÿäº§å°±ç»ªçš„ECGæ•°æ®é›†
    
    ç‰¹æ€§:
    - è‡ªåŠ¨é‡é‡‡æ ·åˆ°ç»Ÿä¸€é‡‡æ ·ç‡ï¼ˆé»˜è®¤500Hzï¼‰
    - æ•°æ®éªŒè¯å’Œé”™è¯¯æ¢å¤
    - å¯é€‰çš„å†…å­˜ç¼“å­˜
    """
    def __init__(self,
                 sim_root_dir: str,
                 csv_root_dir: str,
                 target_size: Tuple[int, int] = (512, 672),
                 target_fs: int = 500,
                 max_samples: Optional[int] = None,
                 cache_in_memory: bool = False,
                 split: str = 'train'):
        """
        Args:
            sim_root_dir: ä»¿çœŸæ•°æ®æ ¹ç›®å½•
            csv_root_dir: åŸå§‹CSVæ•°æ®æ ¹ç›®å½•
            target_size: ç»Ÿä¸€resizeå°ºå¯¸ (H, W)
            target_fs: ç›®æ ‡é‡‡æ ·ç‡ï¼ˆæ‰€æœ‰ä¿¡å·é‡é‡‡æ ·åˆ°æ­¤é‡‡æ ·ç‡ï¼‰
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
            cache_in_memory: æ˜¯å¦ç¼“å­˜åˆ°å†…å­˜ï¼ˆéœ€è¦è¶³å¤ŸRAMï¼‰
            split: 'train' æˆ– 'val'
        """
        self.sim_root = Path(sim_root_dir)
        self.csv_root = Path(csv_root_dir)
        self.target_size = target_size
        self.target_fs = target_fs
        self.split = split
        self.cache_in_memory = cache_in_memory
        
        # æ‰«ææ‰€æœ‰æœ‰æ•ˆæ ·æœ¬
        self.samples = self._scan_samples(max_samples)
        
        # å†…å­˜ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
        self.cache = {} if cache_in_memory else None
        
        # æ•°æ®å˜æ¢ï¼ˆä»…å½’ä¸€åŒ–ï¼Œä»¿çœŸå™¨å·²åšé€€åŒ–ï¼‰
        self.transform = A.Compose([
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2()
        ])
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._print_statistics()
    
    def _scan_samples(self, max_samples: Optional[int]) -> list:
        """æ‰«æå¹¶éªŒè¯æ‰€æœ‰æ ·æœ¬"""
        samples = []
        
        for var_dir in self.sim_root.iterdir():
            if not var_dir.is_dir():
                continue
            
            variation_id = var_dir.name
            
            # æ£€æŸ¥å¿…éœ€æ–‡ä»¶
            required_files = [
                f"{variation_id}_dirty.png",
                f"{variation_id}_label_wave.png",
                f"{variation_id}_label_baseline.npy",
                f"{variation_id}_label_grid.png",
                f"{variation_id}_metadata.json"
            ]
            
            if not all((var_dir / f).exists() for f in required_files):
                warnings.warn(f"æ ·æœ¬ {variation_id} æ–‡ä»¶ä¸å®Œæ•´ï¼Œè·³è¿‡")
                continue
            
            # åŠ è½½å…ƒæ•°æ®éªŒè¯
            try:
                with open(var_dir / f"{variation_id}_metadata.json", 'r') as f:
                    metadata = json.load(f)
                
                # éªŒè¯å…³é”®å­—æ®µ
                if not all(k in metadata for k in ['ecg_id', 'fs', 'sig_len']):
                    warnings.warn(f"æ ·æœ¬ {variation_id} å…ƒæ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡")
                    continue
                
                samples.append({
                    'var_dir': var_dir,
                    'variation_id': variation_id,
                    'metadata': metadata
                })
                
                if max_samples and len(samples) >= max_samples:
                    break
                    
            except Exception as e:
                warnings.warn(f"æ ·æœ¬ {variation_id} å…ƒæ•°æ®è¯»å–å¤±è´¥: {e}")
                continue
        
        if len(samples) == 0:
            raise RuntimeError(f"åœ¨ {self.sim_root} æœªæ‰¾åˆ°æœ‰æ•ˆæ ·æœ¬ï¼")
        
        return samples
    
    def _print_statistics(self):
        """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*70}")
        print(f"ECGæ•°æ®é›†åŠ è½½å®Œæˆ (split={self.split})")
        print(f"{'='*70}")
        print(f"æ€»æ ·æœ¬æ•°: {len(self.samples)}")
        print(f"ç›®æ ‡å°ºå¯¸: {self.target_size[0]}Ã—{self.target_size[1]}")
        print(f"ç›®æ ‡é‡‡æ ·ç‡: {self.target_fs} Hz")
        
        # ç»Ÿè®¡åŸå§‹é‡‡æ ·ç‡åˆ†å¸ƒ
        fs_counts = {}
        layout_counts = {}
        deg_counts = {}
        
        for sample in self.samples:
            fs = sample['metadata']['fs']
            layout = sample['metadata']['layout_type']
            deg = sample['metadata']['degradation_type']
            
            fs_counts[fs] = fs_counts.get(fs, 0) + 1
            layout_counts[layout] = layout_counts.get(layout, 0) + 1
            deg_counts[deg] = deg_counts.get(deg, 0) + 1
        
        print(f"\nåŸå§‹é‡‡æ ·ç‡åˆ†å¸ƒ:")
        for fs, count in sorted(fs_counts.items()):
            pct = count / len(self.samples) * 100
            status = "â†’ é‡é‡‡æ ·" if fs != self.target_fs else "âœ“ ä¿æŒ"
            print(f"  {fs:3d}Hz: {count:5d} ({pct:5.1f}%) {status}")
        
        print(f"\nå¸ƒå±€ç±»å‹åˆ†å¸ƒ:")
        for layout, count in sorted(layout_counts.items()):
            pct = count / len(self.samples) * 100
            print(f"  {layout:15s}: {count:5d} ({pct:5.1f}%)")
        
        print(f"\né€€åŒ–ç±»å‹åˆ†å¸ƒ:")
        for deg, count in sorted(deg_counts.items()):
            pct = count / len(self.samples) * 100
            print(f"  {deg:15s}: {count:5d} ({pct:5.1f}%)")
        
        if self.cache_in_memory:
            estimated_memory = len(self.samples) * 10  # ç²—ç•¥ä¼°ç®— ~10MB/sample
            print(f"\nâš ï¸  å†…å­˜ç¼“å­˜å·²å¯ç”¨ï¼Œé¢„è®¡å ç”¨ ~{estimated_memory}MB RAM")
        
        print(f"{'='*70}\n")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """åŠ è½½å•ä¸ªæ ·æœ¬"""
        # æ£€æŸ¥ç¼“å­˜
        if self.cache_in_memory and idx in self.cache:
            return self.cache[idx]
        
        sample_info = self.samples[idx]
        var_dir = sample_info['var_dir']
        variation_id = sample_info['variation_id']
        metadata = sample_info['metadata']
        
        try:
            # 1. åŠ è½½å›¾åƒæ•°æ®
            dirty_img = cv2.imread(str(var_dir / f"{variation_id}_dirty.png"))
            wave_seg = cv2.imread(str(var_dir / f"{variation_id}_label_wave.png"), cv2.IMREAD_GRAYSCALE)
            baseline_heatmaps = np.load(str(var_dir / f"{variation_id}_label_baseline.npy"))
            grid_mask = cv2.imread(str(var_dir / f"{variation_id}_label_grid.png"), cv2.IMREAD_GRAYSCALE)
            
            # å‡ ä½•å˜æ¢ï¼ˆå¯é€‰ï¼‰
            geometric_transform = metadata.get('geometric_transform', None)
            if geometric_transform is not None:
                theta_gt = torch.tensor(geometric_transform, dtype=torch.float32)[:2, :]
            else:
                theta_gt = torch.tensor([[1, 0, 0], [0, 1, 0]], dtype=torch.float32)
            
            # 2. Resizeåˆ°ç»Ÿä¸€å°ºå¯¸
            h_target, w_target = self.target_size
            
            dirty_resized = cv2.resize(dirty_img, (w_target, h_target), interpolation=cv2.INTER_LINEAR)
            wave_seg_resized = cv2.resize(wave_seg, (w_target, h_target), interpolation=cv2.INTER_NEAREST)
            grid_mask_resized = cv2.resize(grid_mask, (w_target, h_target), interpolation=cv2.INTER_NEAREST)
            
            baseline_resized = np.zeros((12, h_target, w_target), dtype=np.float32)
            for i in range(12):
                baseline_resized[i] = cv2.resize(baseline_heatmaps[i], (w_target, h_target), interpolation=cv2.INTER_LINEAR)
            
            # 3. åŠ è½½åŸå§‹ä¿¡å·
            ecg_id = metadata['ecg_id']
            original_fs = metadata['fs']
            sig_len = metadata['sig_len']
            
            csv_path = self.csv_root / ecg_id / f"{ecg_id}.csv"
            gt_signal = self._load_and_resample_signal(csv_path, original_fs, sig_len, self.target_fs)

            signal_mask = create_signal_mask_from_csv(csv_path, sig_len, num_leads=12)
            
            # 4. æ•°æ®å˜æ¢
            transformed = self.transform(image=dirty_resized)
            image = transformed['image']
            
            wave_seg_tensor = torch.from_numpy(wave_seg_resized).long()
            grid_mask_tensor = torch.from_numpy(grid_mask_resized).float() / 255.0
            baseline_tensor = torch.from_numpy(baseline_resized).float()
            
            result = {
                'image': image,
                'wave_seg': wave_seg_tensor,
                'grid_mask': grid_mask_tensor.unsqueeze(0),
                'baseline_heatmaps': baseline_tensor,
                'theta_gt': theta_gt,
                'gt_signal': gt_signal,
                'metadata': {
                    'variation_id': variation_id,
                    'ecg_id': ecg_id,
                    'original_fs': original_fs,
                    'target_fs': self.target_fs,
                    'physical_params': metadata['physical_params']
                }
            }
            
            # ç¼“å­˜åˆ°å†…å­˜
            if self.cache_in_memory:
                self.cache[idx] = result
            
            return result
            
        except Exception as e:
            warnings.warn(f"åŠ è½½æ ·æœ¬ {variation_id} å¤±è´¥: {e}ï¼Œè¿”å›ç©ºæ ·æœ¬")
            # è¿”å›ä¸€ä¸ªdummyæ ·æœ¬é¿å…è®­ç»ƒä¸­æ–­
            return self._get_dummy_sample()
    
    def _load_and_resample_signal(self, csv_path: Path, original_fs: int, sig_len: int, target_fs: int) -> torch.Tensor:
        """
        åŠ è½½å¹¶é‡é‡‡æ ·ä¿¡å·åˆ°ç›®æ ‡é‡‡æ ·ç‡
        
        ğŸ”¥ ä¿®å¤ï¼šæ­£ç¡®å¤„ç†CSVä¸­çš„NaNå€¼
        - é•¿å¯¼è”ï¼ˆå¦‚IIï¼‰: å®Œæ•´10ç§’æ•°æ®
        - çŸ­å¯¼è”: åªæœ‰éƒ¨åˆ†æ—¶é—´æ®µæœ‰æ•°æ®ï¼Œå…¶ä½™ä¸ºNaN
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            original_fs: åŸå§‹é‡‡æ ·ç‡
            sig_len: åŸå§‹ä¿¡å·é•¿åº¦
            target_fs: ç›®æ ‡é‡‡æ ·ç‡
        
        Returns:
            signal: (T, 12) tensorï¼ŒT = target_fs * 10
        """
        df = pd.read_csv(csv_path)
        
        # æå–12å¯¼è”
        leads = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6']
        signal_list = []
        
        for lead in leads:
            if lead in df.columns:
                sig = df[lead].values[:sig_len]
                
                # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå¤„ç†NaNå€¼
                # æ–¹æ¡ˆ1: å°†NaNæ›¿æ¢ä¸º0ï¼ˆè¡¨ç¤ºè¯¥æ—¶é—´æ®µæ²¡æœ‰ä¿¡å·ï¼‰
                sig = np.nan_to_num(sig, nan=0.0)
                
                # å¦‚æœä¿¡å·ä¸è¶³é•¿åº¦ï¼Œå¡«å……0
                if len(sig) < sig_len:
                    sig = np.pad(sig, (0, sig_len - len(sig)), mode='constant', constant_values=0)
            else:
                # å¯¼è”å®Œå…¨ä¸å­˜åœ¨ï¼Œç”¨å…¨0
                sig = np.zeros(sig_len)
            
            signal_list.append(sig)
        
        signal = np.stack(signal_list, axis=1)  # (T_original, 12)
        
        # ğŸ”¥ äºŒæ¬¡æ£€æŸ¥ï¼šç¡®ä¿æ²¡æœ‰NaNè¿›å…¥é‡é‡‡æ ·
        if np.isnan(signal).any():
            warnings.warn(f"CSV {csv_path.name} ä¸­å‘ç°NaNï¼Œå·²æ›¿æ¢ä¸º0")
            signal = np.nan_to_num(signal, nan=0.0)
        
        # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
        if original_fs != target_fs:
            signal_tensor = torch.from_numpy(signal).float().unsqueeze(0)  # (1, T_original, 12)
            signal_tensor = signal_tensor.permute(0, 2, 1)  # (1, 12, T_original)
            
            target_length = target_fs * 10  # 10ç§’
            signal_resampled = F.interpolate(
                signal_tensor,
                size=target_length,
                mode='linear',
                align_corners=True
            )  # (1, 12, target_length)
            
            signal_resampled = signal_resampled.permute(0, 2, 1).squeeze(0)  # (target_length, 12)
        else:
            signal_resampled = torch.from_numpy(signal).float()
        
        # ğŸ”¥ æœ€ç»ˆæ£€æŸ¥ï¼šç¡®ä¿è¾“å‡ºæ²¡æœ‰NaN
        if torch.isnan(signal_resampled).any():
            warnings.warn(f"é‡é‡‡æ ·åå‘ç°NaNï¼Œå¼ºåˆ¶æ›¿æ¢ä¸º0")
            signal_resampled = torch.nan_to_num(signal_resampled, nan=0.0)
        
        return signal_resampled
    
    def _get_dummy_sample(self) -> Dict[str, torch.Tensor]:
        """è¿”å›ä¸€ä¸ªdummyæ ·æœ¬ï¼ˆç”¨äºé”™è¯¯æ¢å¤ï¼‰"""
        h, w = self.target_size
        return {
            'image': torch.zeros(3, h, w),
            'wave_seg': torch.zeros(h, w, dtype=torch.long),
            'grid_mask': torch.zeros(1, h, w),
            'baseline_heatmaps': torch.zeros(12, h, w),
            'theta_gt': torch.eye(2, 3),
            'gt_signal': torch.zeros(self.target_fs * 10, 12),
            'metadata': {'variation_id': 'dummy'}
        }


def create_dataloaders(sim_root: str,
                       csv_root: str,
                       batch_size: int = 4,
                       num_workers: int = 4,
                       train_split: float = 0.9,
                       target_fs: int = 500,
                       max_samples: Optional[int] = None,
                       cache_in_memory: bool = False) -> Tuple[torch.utils.data.DataLoader, torch.utils.data.DataLoader]:
    """
    åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯DataLoaderçš„ä¾¿æ·å‡½æ•°
    
    Returns:
        train_loader, val_loader
    """
    # åˆ›å»ºå®Œæ•´æ•°æ®é›†
    full_dataset = ECGProductionDataset(
        sim_root_dir=sim_root,
        csv_root_dir=csv_root,
        target_size=(512, 672),
        target_fs=target_fs,
        max_samples=max_samples,
        cache_in_memory=cache_in_memory,
        split='train'
    )
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    total_size = len(full_dataset)
    train_size = int(total_size * train_split)
    val_size = total_size - train_size
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # åˆ›å»ºDataLoader
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=(num_workers > 0),  # ä¿æŒworkerè¿›ç¨‹
        prefetch_factor=2 if num_workers > 0 else None  # é¢„åŠ è½½2ä¸ªbatch
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=(num_workers > 0),
        prefetch_factor=2 if num_workers > 0 else None
    )
    
    print(f"è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"Batch size: {batch_size}")
    print(f"Workers: {num_workers}")
    print()
    
    return train_loader, val_loader


# ========== æµ‹è¯•ä»£ç  ==========

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='æµ‹è¯•æ•°æ®é›†åŠ è½½')
    parser.add_argument('--sim_root', type=str, required=True)
    parser.add_argument('--csv_root', type=str, required=True)
    parser.add_argument('--max_samples', type=int, default=100)
    parser.add_argument('--cache', action='store_true', help='å¯ç”¨å†…å­˜ç¼“å­˜')
    
    args = parser.parse_args()
    
    print("\n" + "="*70)
    print("æ•°æ®é›†åŠ è½½æµ‹è¯•")
    print("="*70)
    
    # åˆ›å»ºDataLoader
    train_loader, val_loader = create_dataloaders(
        sim_root=args.sim_root,
        csv_root=args.csv_root,
        batch_size=4,
        num_workers=2,
        target_fs=500,
        max_samples=args.max_samples,
        cache_in_memory=args.cache
    )
    
    # æµ‹è¯•åŠ è½½é€Ÿåº¦
    import time
    
    print("æµ‹è¯•è®­ç»ƒé›†åŠ è½½é€Ÿåº¦...")
    start = time.time()
    
    for i, batch in enumerate(train_loader):
        if i >= 10:  # åªæµ‹è¯•10ä¸ªbatch
            break
        
        print(f"Batch {i+1}:")
        print(f"  Image: {batch['image'].shape}")
        print(f"  Signal: {batch['gt_signal'].shape}")
        print(f"  åŸå§‹fs: {batch['metadata']['original_fs']}")
        
        # éªŒè¯é‡é‡‡æ ·æ­£ç¡®æ€§
        for j in range(len(batch['gt_signal'])):
            expected_length = batch['metadata']['target_fs'][j] * 10
            actual_length = batch['gt_signal'][j].shape[0]
            assert actual_length == expected_length, f"ä¿¡å·é•¿åº¦ä¸åŒ¹é…: {actual_length} vs {expected_length}"
    
    elapsed = time.time() - start
    print(f"\nâœ“ åŠ è½½10ä¸ªbatchè€—æ—¶: {elapsed:.2f}ç§’ (å¹³å‡ {elapsed/10:.3f}s/batch)")
    
    # æµ‹è¯•ç¼“å­˜æ•ˆæœ
    if args.cache:
        print("\næµ‹è¯•ç¼“å­˜æ•ˆæœï¼ˆç¬¬äºŒæ¬¡åŠ è½½ï¼‰...")
        start = time.time()
        for i, batch in enumerate(train_loader):
            if i >= 10:
                break
        elapsed2 = time.time() - start
        print(f"âœ“ ç¬¬äºŒæ¬¡åŠ è½½10ä¸ªbatchè€—æ—¶: {elapsed2:.2f}ç§’")
        print(f"âœ“ åŠ é€Ÿæ¯”: {elapsed/elapsed2:.2f}x")
    
    print("\n" + "="*70)
    print("âœ“ æ•°æ®é›†æµ‹è¯•é€šè¿‡ï¼")
    print("="*70)