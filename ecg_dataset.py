"""
ECG V45 ç”Ÿäº§å°±ç»ªæ•°æ®é›†åŠ è½½å™¨

ç‰¹ç‚¹:
1. âœ… æ”¯æŒå¤šé‡‡æ ·ç‡ï¼ˆé‡é‡‡æ ·åˆ°500Hzç»Ÿä¸€å¤„ç†ï¼‰
2. âœ… ç›´æ¥ä»ä»¿çœŸå™¨è¾“å‡ºåŠ è½½V45æ ¼å¼
3. âœ… é«˜æ•ˆç¼“å­˜æœºåˆ¶ï¼ˆå¯é€‰ï¼‰
4. âœ… å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ•°æ®éªŒè¯
5. âœ… æ”¯æŒæ‰€æœ‰V45æ ‡æ³¨ï¼ˆçº¸é€ŸOCRã€å¢ç›ŠOCRã€ç‰©ç†çº¦æŸï¼‰
"""

import os
import json
import numpy as np
import cv2
from pathlib import Path
import torch
from torch.utils.data import Dataset
import torch.nn.functional as F
import albumentations as A
from albumentations.pytorch import ToTensorV2
import pandas as pd
from typing import Optional, Tuple, Dict, List
import warnings
from PIL import Image


class ECGV45ProductionDataset(Dataset):
    """
    ECG V45 ç”Ÿäº§å°±ç»ªæ•°æ®é›†
    
    ç‰¹æ€§:
    - è‡ªåŠ¨é‡é‡‡æ ·åˆ°ç»Ÿä¸€é‡‡æ ·ç‡ï¼ˆé»˜è®¤500Hzï¼‰
    - å®Œæ•´æ”¯æŒV45æ ‡æ³¨æ ¼å¼
    - æ•°æ®éªŒè¯å’Œé”™è¯¯æ¢å¤
    - å¯é€‰çš„å†…å­˜ç¼“å­˜
    """
    
    LEAD_NAMES = ['I', 'II', 'III', 'aVR', 'aVL', 'aVF', 
                  'V1', 'V2', 'V3', 'V4', 'V5', 'V6']
    
    def __init__(self,
                 sim_root_dir: str,
                 csv_root_dir: str,
                 split: str = 'train',
                 target_size: Tuple[int, int] = (512, 672),
                 target_fs: int = 500,
                 max_samples: Optional[int] = None,
                 cache_in_memory: bool = False,
                 load_fine_labels: bool = True,
                 load_ocr_labels: bool = True,
                 augment: bool = False):
        """
        Args:
            sim_root_dir: ä»¿çœŸæ•°æ®æ ¹ç›®å½•
            csv_root_dir: åŸå§‹CSVæ•°æ®æ ¹ç›®å½•
            split: 'train' æˆ– 'val' æˆ– 'test'
            target_size: ç»Ÿä¸€resizeå°ºå¯¸ (H, W)
            target_fs: ç›®æ ‡é‡‡æ ·ç‡ï¼ˆæ‰€æœ‰ä¿¡å·é‡é‡‡æ ·åˆ°æ­¤é‡‡æ ·ç‡ï¼‰
            max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰
            cache_in_memory: æ˜¯å¦ç¼“å­˜åˆ°å†…å­˜ï¼ˆéœ€è¦è¶³å¤ŸRAMï¼‰
            load_fine_labels: æ˜¯å¦åŠ è½½ç»†ç²’åº¦æ ‡æ³¨
            load_ocr_labels: æ˜¯å¦åŠ è½½OCRæ ‡æ³¨ï¼ˆçº¸é€Ÿã€å¢ç›Šï¼‰
            augment: æ˜¯å¦ä½¿ç”¨æ•°æ®å¢å¼ºï¼ˆä»…è®­ç»ƒæ—¶ï¼‰
        """
        self.sim_root = Path(sim_root_dir)
        self.csv_root = Path(csv_root_dir)
        self.split = split
        self.target_size = target_size
        self.target_fs = target_fs
        self.cache_in_memory = cache_in_memory
        self.load_fine = load_fine_labels
        self.load_ocr = load_ocr_labels
        self.augment = augment and (split == 'train')
        
        # æ‰«ææ‰€æœ‰æœ‰æ•ˆæ ·æœ¬
        self.samples = self._scan_samples(max_samples)
        
        # å†…å­˜ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
        self.cache = {} if cache_in_memory else None
        
        # æ•°æ®å˜æ¢
        self.transform = self._build_transform()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self._print_statistics()
    
    def _build_transform(self):
        """æ„å»ºæ•°æ®å˜æ¢pipeline"""
        if self.augment:
            # è®­ç»ƒæ—¶å¢å¼º
            transform = A.Compose([
                A.OneOf([
                    A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),
                    A.ISONoise(p=0.5),
                ], p=0.3),
                A.OneOf([
                    A.Blur(blur_limit=3, p=0.5),
                    A.GaussianBlur(blur_limit=3, p=0.5),
                ], p=0.2),
                A.OneOf([
                    A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.5),
                    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=15, val_shift_limit=10, p=0.5),
                ], p=0.3),
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ])
        else:
            # éªŒè¯/æµ‹è¯•æ—¶åªå½’ä¸€åŒ–
            transform = A.Compose([
                A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
                ToTensorV2()
            ])
        
        return transform
    
    def _scan_samples(self, max_samples: Optional[int]) -> List[Dict]:
        """æ‰«æå¹¶éªŒè¯æ‰€æœ‰æ ·æœ¬"""
        samples = []
        
        for sample_dir in sorted(self.sim_root.iterdir()):
            if not sample_dir.is_dir():
                continue
            
            sample_id = sample_dir.name
            
            # V45å¿…éœ€æ–‡ä»¶
            required_files = [
                f"{sample_id}_dirty.png",
                f"{sample_id}_label_wave.png",
                f"{sample_id}_label_baseline_coarse.npy",
                f"{sample_id}_metadata.json"
            ]
            
            # æ£€æŸ¥ç»†ç²’åº¦æ ‡æ³¨
            if self.load_fine:
                required_files.extend([
                    f"{sample_id}_label_baseline_fine.npy",
                    f"{sample_id}_label_text_multi.npy",
                    f"{sample_id}_label_auxiliary.npy",
                    f"{sample_id}_label_grid_fine.npy"
                ])
            
            # æ£€æŸ¥OCRæ ‡æ³¨ï¼ˆV45æ–°å¢ï¼‰
            if self.load_ocr:
                required_files.extend([
                    f"{sample_id}_label_paper_speed.npy",
                    f"{sample_id}_label_gain.npy"
                ])
            
            # éªŒè¯æ–‡ä»¶å®Œæ•´æ€§
            if not all((sample_dir / f).exists() for f in required_files):
                missing = [f for f in required_files if not (sample_dir / f).exists()]
                warnings.warn(f"æ ·æœ¬ {sample_id} ç¼ºå¤±æ–‡ä»¶: {missing}ï¼Œè·³è¿‡")
                continue
            
            # åŠ è½½å¹¶éªŒè¯å…ƒæ•°æ®
            try:
                metadata_path = sample_dir / f"{sample_id}_metadata.json"
                with open(metadata_path, 'r') as f:
                    metadata = json.load(f)
                
                # éªŒè¯å…³é”®å­—æ®µ
                required_keys = ['ecg_id', 'fs', 'sig_len', 'physical_params']
                if not all(k in metadata for k in required_keys):
                    warnings.warn(f"æ ·æœ¬ {sample_id} å…ƒæ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡")
                    continue
                
                samples.append({
                    'sample_dir': sample_dir,
                    'sample_id': sample_id,
                    'metadata': metadata
                })
                
                if max_samples and len(samples) >= max_samples:
                    break
                    
            except Exception as e:
                warnings.warn(f"æ ·æœ¬ {sample_id} å…ƒæ•°æ®è¯»å–å¤±è´¥: {e}")
                continue
        
        if len(samples) == 0:
            raise RuntimeError(f"åœ¨ {self.sim_root} æœªæ‰¾åˆ°æœ‰æ•ˆæ ·æœ¬ï¼")
        
        return samples
    
    def _print_statistics(self):
        """æ‰“å°æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*80}")
        print(f"ECG V45 æ•°æ®é›†åŠ è½½å®Œæˆ (split={self.split})")
        print(f"{'='*80}")
        print(f"æ€»æ ·æœ¬æ•°: {len(self.samples)}")
        print(f"ç›®æ ‡å°ºå¯¸: {self.target_size[0]}Ã—{self.target_size[1]}")
        print(f"ç›®æ ‡é‡‡æ ·ç‡: {self.target_fs} Hz")
        print(f"ç»†ç²’åº¦æ ‡æ³¨: {'âœ“' if self.load_fine else 'âœ—'}")
        print(f"OCRæ ‡æ³¨: {'âœ“' if self.load_ocr else 'âœ—'}")
        print(f"æ•°æ®å¢å¼º: {'âœ“' if self.augment else 'âœ—'}")
        
        # ç»Ÿè®¡åˆ†å¸ƒ
        fs_counts = {}
        layout_counts = {}
        deg_counts = {}
        paper_speed_counts = {}
        gain_counts = {}
        corruption_counts = {'none': 0, 'has_corruption': 0}
        
        for sample in self.samples:
            meta = sample['metadata']
            
            # é‡‡æ ·ç‡
            fs = meta['fs']
            fs_counts[fs] = fs_counts.get(fs, 0) + 1
            
            # å¸ƒå±€
            layout = meta['layout_type']
            layout_counts[layout] = layout_counts.get(layout, 0) + 1
            
            # é€€åŒ–ç±»å‹
            deg = meta['degradation_type']
            deg_counts[deg] = deg_counts.get(deg, 0) + 1
            
            # ç‰©ç†å‚æ•°
            paper_speed = meta['physical_params']['paper_speed_mm_s']
            paper_speed_counts[paper_speed] = paper_speed_counts.get(paper_speed, 0) + 1
            
            gain = meta['physical_params']['gain_mm_mv']
            gain_counts[gain] = gain_counts.get(gain, 0) + 1
            
            # å¯¼è”æ±¡æŸï¼ˆV45ï¼‰
            if 'lead_corruption' in meta and len(meta['lead_corruption']) > 0:
                corruption_counts['has_corruption'] += 1
            else:
                corruption_counts['none'] += 1
        
        print(f"\nåŸå§‹é‡‡æ ·ç‡åˆ†å¸ƒ:")
        for fs, count in sorted(fs_counts.items()):
            pct = count / len(self.samples) * 100
            status = "â†’ é‡é‡‡æ ·" if fs != self.target_fs else "âœ“ ä¿æŒ"
            print(f"  {fs:3d}Hz: {count:5d} ({pct:5.1f}%) {status}")
        
        print(f"\nå¸ƒå±€ç±»å‹åˆ†å¸ƒ:")
        for layout, count in sorted(layout_counts.items()):
            pct = count / len(self.samples) * 100
            print(f"  {layout:15s}: {count:5d} ({pct:5.1f}%)")
        
        print(f"\né€€åŒ–ç±»å‹åˆ†å¸ƒ (Top 5):")
        for deg, count in sorted(deg_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
            pct = count / len(self.samples) * 100
            print(f"  {deg:15s}: {count:5d} ({pct:5.1f}%)")
        
        print(f"\nçº¸é€Ÿåˆ†å¸ƒ â­â­â­â­â­:")
        for speed, count in sorted(paper_speed_counts.items()):
            pct = count / len(self.samples) * 100
            print(f"  {speed:5.1f} mm/s: {count:5d} ({pct:5.1f}%)")
        
        print(f"\nå¢ç›Šåˆ†å¸ƒ â­â­â­:")
        for gain, count in sorted(gain_counts.items()):
            pct = count / len(self.samples) * 100
            print(f"  {gain:5.1f} mm/mV: {count:5d} ({pct:5.1f}%)")
        
        print(f"\nå¯¼è”æ±¡æŸç»Ÿè®¡ (V45):")
        for status, count in corruption_counts.items():
            pct = count / len(self.samples) * 100
            print(f"  {status:15s}: {count:5d} ({pct:5.1f}%)")
        
        if self.cache_in_memory:
            estimated_memory = len(self.samples) * 15  # ~15MB/sample
            print(f"\nâš ï¸  å†…å­˜ç¼“å­˜å·²å¯ç”¨ï¼Œé¢„è®¡å ç”¨ ~{estimated_memory}MB RAM")
        
        print(f"{'='*80}\n")
    
    def __len__(self):
        return len(self.samples)
    
    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:
        """åŠ è½½å•ä¸ªæ ·æœ¬"""
        # æ£€æŸ¥ç¼“å­˜
        if self.cache_in_memory and idx in self.cache:
            return self.cache[idx]
        
        sample_info = self.samples[idx]
        sample_dir = sample_info['sample_dir']
        sample_id = sample_info['sample_id']
        metadata = sample_info['metadata']
        
        try:
            # ========== 1. åŠ è½½å›¾åƒæ•°æ® ==========
            dirty_path = sample_dir / f"{sample_id}_dirty.png"
            image = np.array(Image.open(dirty_path).convert('RGB'))
            
            # æ³¢å½¢åˆ†å‰²æ©ç 
            wave_mask = np.array(Image.open(sample_dir / f"{sample_id}_label_wave.png"))
            
            # ç²—ç²’åº¦åŸºçº¿
            baseline_coarse = np.load(sample_dir / f"{sample_id}_label_baseline_coarse.npy")
            
            # ========== 2. åŠ è½½ç»†ç²’åº¦æ ‡æ³¨ï¼ˆå¯é€‰ï¼‰==========
            if self.load_fine:
                baseline_fine = np.load(sample_dir / f"{sample_id}_label_baseline_fine.npy")
                text_multi = np.load(sample_dir / f"{sample_id}_label_text_multi.npy")
                auxiliary = np.load(sample_dir / f"{sample_id}_label_auxiliary.npy")
                grid_fine = np.load(sample_dir / f"{sample_id}_label_grid_fine.npy")
            else:
                H, W = image.shape[:2]
                baseline_fine = np.zeros((12, H, W), dtype=np.uint8)
                text_multi = np.zeros((13, H, W), dtype=np.uint8)
                auxiliary = np.zeros((1, H, W), dtype=np.uint8)
                grid_fine = np.zeros((1, H, W), dtype=np.uint8)
            
            # ========== 3. åŠ è½½OCRæ ‡æ³¨ï¼ˆV45ï¼‰==========
            if self.load_ocr:
                paper_speed_mask = np.load(sample_dir / f"{sample_id}_label_paper_speed.npy")
                gain_mask = np.load(sample_dir / f"{sample_id}_label_gain.npy")
            else:
                H, W = image.shape[:2]
                paper_speed_mask = np.zeros((1, H, W), dtype=np.uint8)
                gain_mask = np.zeros((1, H, W), dtype=np.uint8)
            
            # ========== 4. Resizeåˆ°ç»Ÿä¸€å°ºå¯¸ ==========
            h_target, w_target = self.target_size
            
            image_resized = cv2.resize(image, (w_target, h_target), interpolation=cv2.INTER_LINEAR)
            wave_mask_resized = cv2.resize(wave_mask, (w_target, h_target), interpolation=cv2.INTER_NEAREST)
            
            # Resizeæ‰€æœ‰æ©ç 
            baseline_coarse_resized = cv2.resize(
                baseline_coarse[0], (w_target, h_target), interpolation=cv2.INTER_LINEAR
            )[np.newaxis, ...]
            
            baseline_fine_resized = np.zeros((12, h_target, w_target), dtype=np.float32)
            for i in range(12):
                baseline_fine_resized[i] = cv2.resize(
                    baseline_fine[i], (w_target, h_target), interpolation=cv2.INTER_LINEAR
                )
            
            text_multi_resized = np.zeros((13, h_target, w_target), dtype=np.float32)
            for i in range(13):
                text_multi_resized[i] = cv2.resize(
                    text_multi[i], (w_target, h_target), interpolation=cv2.INTER_LINEAR
                )
            
            auxiliary_resized = cv2.resize(
                auxiliary[0], (w_target, h_target), interpolation=cv2.INTER_LINEAR
            )[np.newaxis, ...]
            
            grid_fine_resized = cv2.resize(
                grid_fine[0], (w_target, h_target), interpolation=cv2.INTER_NEAREST
            )[np.newaxis, ...]
            
            paper_speed_resized = cv2.resize(
                paper_speed_mask[0], (w_target, h_target), interpolation=cv2.INTER_LINEAR
            )[np.newaxis, ...]
            
            gain_resized = cv2.resize(
                gain_mask[0], (w_target, h_target), interpolation=cv2.INTER_LINEAR
            )[np.newaxis, ...]
            
            # ========== 5. åŠ è½½åŸå§‹GTä¿¡å· ==========
            ecg_id = metadata['ecg_id']
            original_fs = metadata['fs']
            sig_len = metadata['sig_len']
            
            csv_path = self.csv_root / ecg_id / f"{ecg_id}.csv"
            gt_signal = self._load_and_resample_signal(
                csv_path, original_fs, sig_len, self.target_fs
            )
            
            # ========== 6. æ•°æ®å˜æ¢ ==========
            transformed = self.transform(image=image_resized)
            image_tensor = transformed['image']
            
            # ========== 7. è½¬æ¢ä¸ºTensor ==========
            # æ„å»ºæ ‡å‡†çš„lead_roisï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            lead_rois = metadata.get('lead_rois', {})
            if not lead_rois:
                # åˆ›å»ºé»˜è®¤çš„lead_rois
                lead_rois = self._create_default_lead_rois(metadata)
            
            result = {
                # å›¾åƒ
                'image': image_tensor,
                
                # æ³¢å½¢åˆ†å‰²
                'wave_mask': torch.from_numpy(wave_mask_resized).long(),
                
                # åŸºçº¿æ ‡æ³¨
                'baseline_coarse': torch.from_numpy(baseline_coarse_resized).float() / 255.0,
                'baseline_fine': torch.from_numpy(baseline_fine_resized).float() / 255.0,
                
                # æ–‡å­—å’Œè¾…åŠ©
                'text_multi': torch.from_numpy(text_multi_resized).float() / 255.0,
                'auxiliary': torch.from_numpy(auxiliary_resized).float() / 255.0,
                'grid_fine': torch.from_numpy(grid_fine_resized).float() / 255.0,
                
                # OCRæ ‡æ³¨ï¼ˆV45ï¼‰
                'paper_speed_mask': torch.from_numpy(paper_speed_resized).float() / 255.0,
                'gain_mask': torch.from_numpy(gain_resized).float() / 255.0,
                
                # GTä¿¡å·
                'gt_signal': gt_signal,
                
                # å…ƒæ•°æ®
                'metadata': {
                    'sample_id': sample_id,
                    'ecg_id': ecg_id,
                    'original_fs': original_fs,
                    'target_fs': self.target_fs,
                    'physical_params': metadata['physical_params'],
                    'lead_rois': lead_rois,
                    'ocr_targets': metadata.get('ocr_targets', {}),
                    'lead_corruption': metadata.get('lead_corruption', {}),
                }
            }
            
            # ç¼“å­˜åˆ°å†…å­˜
            if self.cache_in_memory:
                self.cache[idx] = result
            
            return result
            
        except Exception as e:
            warnings.warn(f"åŠ è½½æ ·æœ¬ {sample_id} å¤±è´¥: {e}ï¼Œè¿”å›dummyæ ·æœ¬")
            return self._get_dummy_sample()
    
    def _load_and_resample_signal(self, csv_path: Path, 
                                  original_fs: int, 
                                  sig_len: int, 
                                  target_fs: int) -> torch.Tensor:
        """
        åŠ è½½å¹¶é‡é‡‡æ ·ä¿¡å·åˆ°ç›®æ ‡é‡‡æ ·ç‡
        
        å…³é”®ä¿®å¤ï¼šæ­£ç¡®å¤„ç†CSVä¸­çš„NaNå€¼
        - é•¿å¯¼è”ï¼ˆå¦‚IIï¼‰: å®Œæ•´10ç§’æ•°æ®
        - çŸ­å¯¼è”: åªæœ‰éƒ¨åˆ†æ—¶é—´æ®µæœ‰æ•°æ®ï¼Œå…¶ä½™ä¸ºNaN
        
        Args:
            csv_path: CSVæ–‡ä»¶è·¯å¾„
            original_fs: åŸå§‹é‡‡æ ·ç‡
            sig_len: åŸå§‹ä¿¡å·é•¿åº¦
            target_fs: ç›®æ ‡é‡‡æ ·ç‡
        
        Returns:
            signal: (T, 12) tensorï¼ŒT = target_fs * 10
        """
        try:
            df = pd.read_csv(csv_path)
            
            signal_list = []
            
            for lead in self.LEAD_NAMES:
                if lead in df.columns:
                    sig = df[lead].values[:sig_len]
                    
                    # ğŸ”¥ å…³é”®ä¿®å¤ï¼šå°†NaNæ›¿æ¢ä¸º0
                    sig = np.nan_to_num(sig, nan=0.0)
                    
                    # å¡«å……åˆ°æ ‡å‡†é•¿åº¦
                    if len(sig) < sig_len:
                        sig = np.pad(sig, (0, sig_len - len(sig)), 
                                   mode='constant', constant_values=0)
                else:
                    # å¯¼è”ä¸å­˜åœ¨ï¼Œç”¨å…¨0
                    sig = np.zeros(sig_len)
                
                signal_list.append(sig)
            
            signal = np.stack(signal_list, axis=1)  # (T_original, 12)
            
            # äºŒæ¬¡æ£€æŸ¥NaN
            if np.isnan(signal).any():
                warnings.warn(f"CSV {csv_path.name} ä¸­å‘ç°NaNï¼Œå·²æ›¿æ¢ä¸º0")
                signal = np.nan_to_num(signal, nan=0.0)
            
            # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
            if original_fs != target_fs:
                signal_tensor = torch.from_numpy(signal).float().unsqueeze(0)
                signal_tensor = signal_tensor.permute(0, 2, 1)  # (1, 12, T)
                
                target_length = int(target_fs * 10)  # 10ç§’
                signal_resampled = F.interpolate(
                    signal_tensor,
                    size=target_length,
                    mode='linear',
                    align_corners=True
                )
                
                signal_resampled = signal_resampled.permute(0, 2, 1).squeeze(0)
            else:
                signal_resampled = torch.from_numpy(signal).float()
            
            # æœ€ç»ˆæ£€æŸ¥
            if torch.isnan(signal_resampled).any():
                warnings.warn(f"é‡é‡‡æ ·åå‘ç°NaNï¼Œå¼ºåˆ¶æ›¿æ¢ä¸º0")
                signal_resampled = torch.nan_to_num(signal_resampled, nan=0.0)
            
            return signal_resampled
            
        except Exception as e:
            warnings.warn(f"åŠ è½½ä¿¡å·å¤±è´¥ {csv_path}: {e}ï¼Œè¿”å›é›¶ä¿¡å·")
            target_length = int(target_fs * 10)
            return torch.zeros(target_length, 12)
    
    def _get_dummy_sample(self) -> Dict[str, torch.Tensor]:
        """è¿”å›dummyæ ·æœ¬ï¼ˆç”¨äºé”™è¯¯æ¢å¤ï¼‰"""
        h, w = self.target_size
        target_length = int(self.target_fs * 10)
        
        return {
            'image': torch.zeros(3, h, w),
            'wave_mask': torch.zeros(h, w, dtype=torch.long),
            'baseline_coarse': torch.zeros(1, h, w),
            'baseline_fine': torch.zeros(12, h, w),
            'text_multi': torch.zeros(13, h, w),
            'auxiliary': torch.zeros(1, h, w),
            'grid_fine': torch.zeros(1, h, w),
            'paper_speed_mask': torch.zeros(1, h, w),
            'gain_mask': torch.zeros(1, h, w),
            'gt_signal': torch.zeros(target_length, 12),
            'metadata': {'sample_id': 'dummy', 'ecg_id': 'dummy'}
        }


def create_dataloaders(
    sim_root: str,
    csv_root: str,
    batch_size: int = 4,
    num_workers: int = 4,
    train_split: float = 0.9,
    target_fs: int = 500,
    target_size: Tuple[int, int] = (512, 672),
    max_samples: Optional[int] = None,
    cache_in_memory: bool = False,
    load_fine_labels: bool = True,
    load_ocr_labels: bool = True
) -> Tuple:
    """
    åˆ›å»ºè®­ç»ƒå’ŒéªŒè¯DataLoader
    
    Returns:
        train_loader, val_loader
    """
    # åˆ›å»ºå®Œæ•´æ•°æ®é›†
    full_dataset = ECGV45ProductionDataset(
        sim_root_dir=sim_root,
        csv_root_dir=csv_root,
        split='train',
        target_size=target_size,
        target_fs=target_fs,
        max_samples=max_samples,
        cache_in_memory=cache_in_memory,
        load_fine_labels=load_fine_labels,
        load_ocr_labels=load_ocr_labels,
        augment=True
    )
    
    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
    total_size = len(full_dataset)
    train_size = int(total_size * train_split)
    val_size = total_size - train_size
    
    train_dataset, val_dataset = torch.utils.data.random_split(
        full_dataset,
        [train_size, val_size],
        generator=torch.Generator().manual_seed(42)
    )
    
    # ä¸ºéªŒè¯é›†ç¦ç”¨å¢å¼º
    val_dataset.dataset.augment = False
    
    # åˆ›å»ºDataLoader
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=(num_workers > 0),
        prefetch_factor=2 if num_workers > 0 else None,
        drop_last=True  # é¿å…æœ€åä¸€ä¸ªbatchå¤ªå°
    )
    
    val_loader = torch.utils.data.DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        pin_memory=True,
        persistent_workers=(num_workers > 0),
        prefetch_factor=2 if num_workers > 0 else None
    )
    
    print(f"âœ“ è®­ç»ƒé›†: {len(train_dataset)} æ ·æœ¬")
    print(f"âœ“ éªŒè¯é›†: {len(val_dataset)} æ ·æœ¬")
    print(f"âœ“ Batch size: {batch_size}")
    print(f"âœ“ Workers: {num_workers}\n")
    
    return train_loader, val_loader


# ========== æµ‹è¯•ä»£ç  ==========

if __name__ == "__main__":
    import argparse
    import time
    
    parser = argparse.ArgumentParser(description='æµ‹è¯•ECG V45æ•°æ®é›†')
    parser.add_argument('--sim_root', type=str, required=True, help='ä»¿çœŸæ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--csv_root', type=str, required=True, help='CSVæ•°æ®æ ¹ç›®å½•')
    parser.add_argument('--max_samples', type=int, default=100, help='æœ€å¤§æ ·æœ¬æ•°')
    parser.add_argument('--batch_size', type=int, default=4, help='Batch size')
    parser.add_argument('--num_workers', type=int, default=2, help='Workeræ•°é‡')
    parser.add_argument('--cache', action='store_true', help='å¯ç”¨å†…å­˜ç¼“å­˜')
    
    args = parser.parse_args()
    
    print("\n" + "="*80)
    print("ECG V45 æ•°æ®é›†åŠ è½½æµ‹è¯•")
    print("="*80)
    
    # åˆ›å»ºDataLoader
    train_loader, val_loader = create_dataloaders(
        sim_root=args.sim_root,
        csv_root=args.csv_root,
        batch_size=args.batch_size,
        num_workers=args.num_workers,
        target_fs=500,
        max_samples=args.max_samples,
        cache_in_memory=args.cache,
        load_fine_labels=True,
        load_ocr_labels=True
    )
    
    # æµ‹è¯•åŠ è½½
    print("æµ‹è¯•æ•°æ®åŠ è½½...")
    start = time.time()
    
    for i, batch in enumerate(train_loader):
        if i >= 10:
            break
        
        print(f"\nBatch {i+1}:")
        print(f"  Image: {batch['image'].shape}")
        print(f"  Wave mask: {batch['wave_mask'].shape}")
        print(f"  Baseline coarse: {batch['baseline_coarse'].shape}")
        print(f"  Baseline fine: {batch['baseline_fine'].shape}")
        print(f"  Text multi: {batch['text_multi'].shape}")
        print(f"  Paper speed mask: {batch['paper_speed_mask'].shape} â­â­â­â­â­")
        print(f"  Gain mask: {batch['gain_mask'].shape} â­â­â­")
        print(f"  GT signal: {batch['gt_signal'].shape}")
        
        # éªŒè¯æ•°æ®å®Œæ•´æ€§
        assert not torch.isnan(batch['image']).any(), "Image contains NaN"
        assert not torch.isnan(batch['gt_signal']).any(), "Signal contains NaN"
        assert batch['wave_mask'].max() <= 12, "Wave mask invalid"
        
        # æ£€æŸ¥OCRæ©ç 
        paper_speed_coverage = (batch['paper_speed_mask'] > 0.5).float().mean()
        gain_coverage = (batch['gain_mask'] > 0.5).float().mean()
        print(f"  Paper speed coverage: {paper_speed_coverage.item()*100:.2f}%")
        print(f"  Gain coverage: {gain_coverage.item()*100:.2f}%")
    
    elapsed = time.time() - start
    print(f"\nâœ“ åŠ è½½10ä¸ªbatchè€—æ—¶: {elapsed:.2f}ç§’ (å¹³å‡ {elapsed/10:.3f}s/batch)")
    
    # æµ‹è¯•éªŒè¯é›†
    print("\næµ‹è¯•éªŒè¯é›†...")
    val_batch = next(iter(val_loader))
    print(f"âœ“ éªŒè¯é›†batchå½¢çŠ¶: {val_batch['image'].shape}")
    
    # æµ‹è¯•ç¼“å­˜æ•ˆæœ
    if args.cache:
        print("\næµ‹è¯•ç¼“å­˜æ•ˆæœï¼ˆç¬¬äºŒæ¬¡åŠ è½½ï¼‰...")
        start = time.time()
        for i, batch in enumerate(train_loader):
            if i >= 10:
                break
        elapsed2 = time.time() - start
        print(f"âœ“ ç¬¬äºŒæ¬¡åŠ è½½è€—æ—¶: {elapsed2:.2f}ç§’")
        print(f"âœ“ åŠ é€Ÿæ¯”: {elapsed/elapsed2:.2f}x")
    
    print("\n" + "="*80)
    print("âœ“ æ•°æ®é›†æµ‹è¯•é€šè¿‡ï¼")
    print("="*80)